{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "dd4fec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bb2b6",
   "metadata": {},
   "source": [
    "# GPU/CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "19403797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d6090",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "907fe23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import AvgPooling\n",
    "from dgl.nn.pytorch import GATConv, GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8377788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCT(nn.Module):\n",
    "    def __init__(self, in_dim, gcn_hidden_dim, mlp_hidden_dim, out_dim):\n",
    "        super(GCT, self).__init__()\n",
    "        \n",
    "        # Graph convolution network\n",
    "        self.conv1 = GraphConv(in_dim, gcn_hidden_dim)\n",
    "        self.conv2 = GraphConv(gcn_hidden_dim, mlp_hidden_dim)\n",
    "        \n",
    "        # Average pooling layer\n",
    "        self.avgpool = AvgPooling()\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp = nn.Sequential(nn.Linear(mlp_hidden_dim, mlp_hidden_dim), nn.ReLU(True), nn.Linear(mlp_hidden_dim, out_dim))\n",
    "        \n",
    "    def forward(self, g):\n",
    "        # Feature selection\n",
    "        h = torch.cat([g.ndata['position'],g.ndata['shape'], g.ndata['embedding']],dim=1)\n",
    "        if torch.cuda.is_available(): h = h.cuda()\n",
    "        \n",
    "        # Passing through all layers\n",
    "        h = self.conv1(g,h)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        \n",
    "        h = self.conv2(g,h)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        \n",
    "        h = self.avgpool(g,h)\n",
    "        \n",
    "        h = self.mlp(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29d632",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a188317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from gensim.models import fasttext\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "94328dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S-BERT model path \n",
    "model_path = \"""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "455574fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert = SentenceTransformer(model_path+'all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b8faa5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_idx(label):\n",
    "    # Converts label to index\n",
    "    label_dict = {\n",
    "        'invoice': 0,\n",
    "        'bank_statement': 1,\n",
    "        'payslip': 2\n",
    "    }\n",
    "    \n",
    "    return label_dict[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "44dafe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_fet(file_and_label):\n",
    "    file, label = file_and_label\n",
    "    label = label_to_idx(label)\n",
    "    df = pd.read_excel(file, engine='openpyxl')\n",
    "    \n",
    "    # Document dimensions\n",
    "    doc_width = df['x_max'].max() - df['x_min'].min()\n",
    "    doc_height = df['y_max'].max() - df['y_min'].min()\n",
    "    \n",
    "    # Define networkX graph\n",
    "    nxG = nx.from_pandas_edgelist(df,'Unnamed: 0', 'Unnamed: 0')\n",
    "    \n",
    "    # Instantiate s-bert embeddings\n",
    "    embeddings = SentenceTransformer(model_path+'all-mpnet-base-v2')\n",
    "        \n",
    "    node_positions = []\n",
    "    node_shapes = []\n",
    "    node_embeddings = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Define node properties\n",
    "        node_width = (row['x_max'] - row['x_min'])/doc_width # normalised width of a bounding box\n",
    "        node_height = (row['y_max'] - row['y_min'])/doc_height # normalised height of a bounding box\n",
    "        \n",
    "        # Calculate node shape\n",
    "        node_shape = [node_width, node_height]\n",
    "        node_shapes.append(node_shape)\n",
    "        \n",
    "        # Calculate Node positions\n",
    "        node_position = [row['x_min']/doc_width, row['y_min']/doc_height]\n",
    "        node_positions.append(node_position)\n",
    "        \n",
    "        # Get embedding of the node text\n",
    "        node_embeddings.append(embeddings.encode(str(row['text'])))\n",
    "        \n",
    "        # Add edges\n",
    "        if(row['side_node_idx'] != -1):\n",
    "            nxG.add_edge(int(index), int(row['side_node_idx']))\n",
    "        if(row['below_node_idx'] != -1):\n",
    "            nxG.add_edge(int(index), int(row['below_node_idx']))\n",
    "    \n",
    "    # Convert networkX to DGL\n",
    "    G = dgl.from_networkx(nxG)\n",
    "    G = dgl.to_bidirected(G)\n",
    "    \n",
    "    # Add node properties to nodes\n",
    "    G.ndata['position'] = torch.tensor(node_positions).float()\n",
    "    G.ndata['shape'] = torch.tensor(node_shapes).float()\n",
    "    G.ndata['embedding'] = torch.tensor(node_embeddings)\n",
    "\n",
    "    return G, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "539f30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_and_labels_from_path(root_path):\n",
    "    # From root path it scans through all folders and returns list of files with directory name as its label\n",
    "    dirs = os.listdir(root_path)\n",
    "    files = []\n",
    "    for d in dirs:\n",
    "        all_files_in_d = os.listdir(os.path.join(root_path,d))\n",
    "        all_files_in_d_with_path = map(lambda x:  (os.path.join(os.path.join(root_path,d), x), d), all_files_in_d)\n",
    "        files = files + list(all_files_in_d_with_path)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "af5b3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(data.Dataset):\n",
    "    def __init__(self, root_path, embedding_type='', embedding_model_path=''):\n",
    "        self.files = get_files_and_labels_from_path(root_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return build_graph_from_fet(self.files[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "4c694c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # Input is a list of pairs (G, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graphs = dgl.batch(graphs)\n",
    "    return batched_graphs, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "475d9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_path = 'Data\\\\classification\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c4047b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = GraphDataset(os.path.join(all_files_path, 'train'))\n",
    "test_data = GraphDataset(os.path.join(all_files_path, 'test'))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8530ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(G):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    nx.draw(dgl.to_networkx(G), with_labels = True, node_color='g', node_size=200, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a2e4a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "1c3ddeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "b533acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 772\n",
    "GCN_HIDDEN_DIM = 512\n",
    "MLP_HIDDEN_DIM = 512\n",
    "OUT_DIM = 3\n",
    "\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "58d98666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_label(x):\n",
    "    return list(x).index(max(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "06cc2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_prec_rec_f1(l1, l2):\n",
    "    # Calculate precission, recall and f1 \n",
    "    l1 = l1.cpu().detach().numpy()\n",
    "    l2 = l2.cpu().detach().numpy()\n",
    "    \n",
    "    l2_labels = [get_label(l) for l in l2]\n",
    "    \n",
    "    prec = precision_score(list(l1), list(l2_labels), average='macro')\n",
    "    rec = recall_score(list(l1), list(l2_labels), average='macro')\n",
    "    f1 = f1_score(list(l1), list(l2_labels), average='macro')\n",
    "    \n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "30965f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    precs = []\n",
    "    recs = []\n",
    "    f1s = []\n",
    "    \n",
    "    for iter, (graph, label) in enumerate(test_loader):\n",
    "        graph = graph.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        print('Testing... ',100*float(iter)/len(test_loader),'%',end='\\r')\n",
    "        \n",
    "        predicted_label = model(graph)\n",
    "        \n",
    "        prec, rec, f1 = calculate_prec_rec_f1(label, predicted_label)\n",
    "        \n",
    "        precs.append(prec)\n",
    "        recs.append(rec)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "    mean_prec = sum(precs)/len(precs)\n",
    "    mean_recall = sum(recs)/len(recs)\n",
    "    mean_f1 = sum(f1s)/len(f1s)\n",
    "        \n",
    "    print(f'Precission: {mean_prec} --- Recall: {mean_recall} --- F1 Score: {mean_f1}')\n",
    "    return mean_prec, mean_recall, mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "4a8fc2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "\n",
    "def train(model):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    scheduler = StepLR(optimizer, 5, gamma = 0.9)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_log = open('train_log.txt','w')\n",
    "    train_log.close()\n",
    "    best_f1 = 0\n",
    "    epochs_no_improvement = 0\n",
    "    patience = 20\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        print(\"\\n\\n\")\n",
    "        model.training=True\n",
    "        \n",
    "        for iter, (graph, label) in enumerate(train_loader):\n",
    "            graph = graph.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            progress = 100*float(iter)/len(data_loader)\n",
    "            progress = float(\"{:.2f}\".format(progress))\n",
    "            print('Epoch '+str(epoch)+' '+str(progress)+'%',end=\"\\r\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predicted_label = model(graph)\n",
    "            loss = loss_func(predicted_label, label)\n",
    "            epoch_loss += float(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('\\t* Epoch '+str(epoch) +' epoch loss '+str(float(epoch_loss))+ ' lr ' + str(scheduler.get_lr()[0]))\n",
    "        scheduler.step()\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # Validation step\n",
    "        prec, rec, f1 = test(model)\n",
    "        \n",
    "        # Train log\n",
    "        train_log = open('train_log.txt','a')\n",
    "        train_log.write('\\t Epoch: '+str(epoch) +' train loss: '+str(float(epoch_loss)) + ' val f1 score: ' + str(f1)+'\\n')\n",
    "        train_log.close()\n",
    "        \n",
    "        # Choose to retain model\n",
    "        if(f1 > best_f1):\n",
    "            best_f1 = f1\n",
    "            print('New best score: ',f1)\n",
    "            torch.save(model,'model_gct.pt')\n",
    "            epochs_no_improvement = 0\n",
    "        else:\n",
    "            epochs_no_improvement += 1\n",
    "        \n",
    "        # Stop the training if there is no improvement in accuracy\n",
    "        if epochs_no_improvement > patience:\n",
    "            print('Epochs no improvement',epochs_no_improvement)\n",
    "            print('Training finished')\n",
    "            train_log.close()\n",
    "            break\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "45cd4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCT(INPUT_DIM, GCN_HIDDEN_DIM, MLP_HIDDEN_DIM, OUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "3d5965e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|   Modules    | Parameters |\n",
      "+--------------+------------+\n",
      "| conv1.weight |   395264   |\n",
      "|  conv1.bias  |    512     |\n",
      "| conv2.weight |   262144   |\n",
      "|  conv2.bias  |    512     |\n",
      "| mlp.0.weight |   262144   |\n",
      "|  mlp.0.bias  |    512     |\n",
      "| mlp.2.weight |    1536    |\n",
      "|  mlp.2.bias  |     3      |\n",
      "+--------------+------------+\n",
      "Total Trainable Params: 922627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "922627"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8387e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch 0 42.86%\r"
     ]
    }
   ],
   "source": [
    "model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "14f07291",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [339]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mepoch_losses\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch_losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdf3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
